# Transformer model configuration
model:
  type: "transformer"
  input_size: 1
  output_size: 1
  d_model: 64
  nhead: 8
  num_layers: 3
  dropout: 0.1
  max_seq_len: 1000

training:
  learning_rate: 0.0005
  weight_decay: 1e-5
  max_epochs: 100
  patience: 10
